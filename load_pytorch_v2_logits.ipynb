{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf2gpt.model import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = torch.load('./model-v1/80000/mp_rank_00_model_states.pt', map_location='cpu')\n",
    "m1 = torch.load('./model-v1/80000/mp_rank_01_model_states.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_weight(model, name):\n",
    "    for n, w in model['module'].items():\n",
    "        if name == n:\n",
    "            return w, list(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embeddings.weight torch.Size([15000, 2560])\n",
      "position_embeddings.weight torch.Size([1024, 2560])\n",
      "transformer.layers.0.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.0.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.0.attention.query_key_value.weight torch.Size([3840, 2560])\n",
      "transformer.layers.0.attention.query_key_value.bias torch.Size([3840])\n",
      "transformer.layers.0.attention.dense.weight torch.Size([2560, 1280])\n",
      "transformer.layers.0.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.0.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.0.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.0.mlp.dense_h_to_4h.weight torch.Size([5120, 2560])\n",
      "transformer.layers.0.mlp.dense_h_to_4h.bias torch.Size([5120])\n",
      "transformer.layers.0.mlp.dense_4h_to_h.weight torch.Size([2560, 5120])\n",
      "transformer.layers.0.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.final_layernorm.weight torch.Size([2560])\n",
      "transformer.final_layernorm.bias torch.Size([2560])\n"
     ]
    }
   ],
   "source": [
    "for n, w in m0['module'].items():\n",
    "    if '.layers.' in n:\n",
    "        if '.layers.0.' in n:\n",
    "            print(n, w.shape)\n",
    "    else:\n",
    "        print(n, w.shape)\n",
    "\n",
    "# for n, w in m1['module'].items():\n",
    "#     print(n, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.set_floatx('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(\n",
    "    vocab_size=30_000,\n",
    "    layer_size=32,\n",
    "    block_size=1024,\n",
    "    embedding_dropout=0.0,\n",
    "    embedding_size=2560,\n",
    "    num_attention_heads=32,\n",
    "    attention_dropout=0.0,\n",
    "    residual_dropout=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "strs = tf.TensorSpec(shape=[None, None],\n",
    "                     dtype=tf.int32,\n",
    "                     name=\"input_strs\")\n",
    "\n",
    "gpt._set_inputs(strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 30000)\n"
     ]
    }
   ],
   "source": [
    "print(gpt(tf.constant([[1]])).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in gpt.weights:\n",
    "#     assert x.dtype == tf.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt/embedding/embeddings:0 (30000, 2560)\n",
      "position_embeddings:0 (1024, 2560)\n",
      "gpt/layer00/attention/query_layer/kernel:0 (2560, 2560)\n",
      "gpt/layer00/attention/query_layer/bias:0 (2560,)\n",
      "gpt/layer00/attention/key_layer/kernel:0 (2560, 2560)\n",
      "gpt/layer00/attention/key_layer/bias:0 (2560,)\n",
      "gpt/layer00/attention/value_layer/kernel:0 (2560, 2560)\n",
      "gpt/layer00/attention/value_layer/bias:0 (2560,)\n",
      "gpt/layer00/attention/context_projection_layer/kernel:0 (2560, 2560)\n",
      "gpt/layer00/attention/context_projection_layer/bias:0 (2560,)\n",
      "gpt/layer00/LayerNorm_mlp_ln0/gamma:0 (2560,)\n",
      "gpt/layer00/LayerNorm_mlp_ln0/beta:0 (2560,)\n",
      "gpt/layer00/LayerNorm_mlp_ln1/gamma:0 (2560,)\n",
      "gpt/layer00/LayerNorm_mlp_ln1/beta:0 (2560,)\n",
      "gpt/layer00/intermediate/kernel:0 (2560, 10240)\n",
      "gpt/layer00/intermediate/bias:0 (10240,)\n",
      "gpt/layer00/output/kernel:0 (10240, 2560)\n",
      "gpt/layer00/output/bias:0 (2560,)\n",
      "gpt/LayerNorm_final_norm/gamma:0 (2560,)\n",
      "gpt/LayerNorm_final_norm/beta:0 (2560,)\n"
     ]
    }
   ],
   "source": [
    "for x in gpt.weights:\n",
    "    if 'gpt/layer' in x.name:\n",
    "        if 'gpt/layer00' in x.name:\n",
    "            print(x.name, x.shape)\n",
    "    else:\n",
    "        print(x.name, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_weights = []\n",
    "\n",
    "for x in gpt.weights:\n",
    "    xs = list(x.shape)\n",
    "\n",
    "    if 'gpt/embedding/embeddings:' in x.name:\n",
    "        pname = 'word_embeddings.weight'\n",
    "        w0, ws0 = find_weight(m0, pname)\n",
    "        w1, ws1 = find_weight(m1, pname)\n",
    "        assert ws0 == [15_000, 2560]\n",
    "        assert ws1 == [15_000, 2560]\n",
    "        w = np.concatenate([w0.numpy(), w1.numpy()])\n",
    "        assert w.shape == (3_0000, 2560)\n",
    "        new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "    elif 'position_embeddings' in x.name:\n",
    "        pname = 'position_embeddings.weight'\n",
    "        w0, ws0 = find_weight(m0, pname)\n",
    "        assert xs == ws0\n",
    "        w = w0.numpy()\n",
    "        new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "    elif 'gpt/layer' in x.name:\n",
    "        n_layer = int(x.name[len('gpt/layer'):][:2])\n",
    "        if 'query_layer' in x.name or 'key_layer' in x.name or 'value_layer' in x.name:\n",
    "\n",
    "            if '/kernel' in x.name:\n",
    "                pname = f'transformer.layers.{n_layer}.attention.query_key_value.weight'\n",
    "                w0, ws0 = find_weight(m0, pname)\n",
    "                w1, ws1 = find_weight(m1, pname)\n",
    "                assert ws0 == [3840, 2560]\n",
    "                assert ws1 == [3840, 2560]\n",
    "                w = np.concatenate([w0.numpy(), w1.numpy()])\n",
    "                w = np.transpose(w)\n",
    "                if 'query_layer' in x.name:\n",
    "                    w = np.concatenate([w0.numpy()[:1280, :], w1.numpy()[:1280, :]])\n",
    "                elif 'key_layer' in x.name:\n",
    "                    w = np.concatenate([w0.numpy()[1280:1280*2, :], w1.numpy()[1280:1280*2, :]])\n",
    "                elif 'value_layer' in x.name:\n",
    "                    w = np.concatenate([w0.numpy()[1280*2:, :], w1.numpy()[1280*2:, :]])\n",
    "                w = np.transpose(w)\n",
    "                assert w.shape == (2560, 2560)\n",
    "                new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "            elif '/bias' in x.name:\n",
    "                pname = f'transformer.layers.{n_layer}.attention.query_key_value.bias'\n",
    "                w0, ws0 = find_weight(m0, pname)\n",
    "                w1, ws1 = find_weight(m1, pname)\n",
    "                assert ws0 == [3840,]\n",
    "                assert ws1 == [3840,]\n",
    "                w = np.concatenate([w0.numpy(), w1.numpy()])\n",
    "                if 'query_layer' in x.name:\n",
    "                    w = np.concatenate([w0.numpy()[:1280], w1.numpy()[:1280]])\n",
    "                elif 'key_layer' in x.name:\n",
    "                    w = np.concatenate([w0.numpy()[1280:1280*2], w1.numpy()[1280:1280*2]])\n",
    "                elif 'value_layer' in x.name:\n",
    "                    w = np.concatenate([w0.numpy()[1280*2:], w1.numpy()[1280*2:]])\n",
    "                assert w.shape == (2560,)\n",
    "                new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "        elif 'attention/context_projection_layer/kernel' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.attention.dense.weight'\n",
    "            w0, ws0 = find_weight(m0, pname)\n",
    "            w1, ws1 = find_weight(m1, pname)\n",
    "            w = np.concatenate([w0.numpy(), w1.numpy()], axis=1)\n",
    "            w = np.transpose(w)\n",
    "            assert w.shape == (2560, 2560)\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "        elif 'attention/context_projection_layer/bias' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.attention.dense.bias'\n",
    "            w0, ws0 = find_weight(m0, pname)\n",
    "            w = w0.numpy()\n",
    "            assert w.shape == (2560,)\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "        elif 'LayerNorm_mlp_ln0/gamma' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.input_layernorm.weight'\n",
    "            w0, ws0 = find_weight(m0, pname)\n",
    "            w = w0.numpy()\n",
    "            assert ws0 == xs\n",
    "            new_weights.append((x.name, x.shape, pname, w))\n",
    "\n",
    "        elif 'LayerNorm_mlp_ln1/gamma' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.post_attention_layernorm.weight'\n",
    "            w0, ws0 = find_weight(m0, pname)\n",
    "            w = w0.numpy()\n",
    "            assert ws0 == xs\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "        elif 'LayerNorm_mlp_ln0/beta' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.input_layernorm.bias'\n",
    "            w0, ws0 = find_weight(m0, pname)\n",
    "            w = w0.numpy()\n",
    "            assert ws0 == xs\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "        elif 'LayerNorm_mlp_ln1/beta' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.post_attention_layernorm.bias'\n",
    "            w0, ws0 = find_weight(m0, pname)\n",
    "            w = w0.numpy()\n",
    "            assert ws0 == xs\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "        elif 'intermediate/kernel' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.mlp.dense_h_to_4h.weight'\n",
    "            w0, ws0 = find_weight(m0, pname)\n",
    "            w1, ws1 = find_weight(m1, pname)\n",
    "            w = np.concatenate([w0.numpy(), w1.numpy()], axis=0)\n",
    "            w = np.transpose(w)\n",
    "            assert w.shape == (2560, 10240)\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "        elif 'intermediate/bias' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.mlp.dense_h_to_4h.bias'\n",
    "            w0, ws0 = find_weight(m0, pname)\n",
    "            w1, ws1 = find_weight(m1, pname)\n",
    "            w = np.concatenate([w0.numpy(), w1.numpy()], axis=0)\n",
    "            w = np.transpose(w)\n",
    "            assert w.shape == (10240,)\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "        elif '/output/kernel' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.mlp.dense_4h_to_h.weight'\n",
    "            w0, ws0 = find_weight(m0, pname)\n",
    "            w1, ws1 = find_weight(m1, pname)\n",
    "            w = np.concatenate([w0.numpy(), w1.numpy()], axis=1)\n",
    "            w = np.transpose(w)\n",
    "            assert w.shape == (10240, 2560)\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "        elif '/output/bias' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.mlp.dense_4h_to_h.bias'\n",
    "            w0, ws0 = find_weight(m0, pname)\n",
    "            w = w0.numpy()\n",
    "            assert w.shape == (2560,)\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "        else:\n",
    "            print('BAD', x.name, xs)\n",
    "            break\n",
    "    elif 'gpt/LayerNorm_final_norm/gamma' in x.name:\n",
    "        pname = 'transformer.final_layernorm.weight'\n",
    "        w0, ws0 = find_weight(m0, pname)\n",
    "        w = w0.numpy()\n",
    "        assert ws0 == xs\n",
    "        new_weights.append((x.name, xs, pname, w))\n",
    "    elif 'gpt/LayerNorm_final_norm/beta' in x.name:\n",
    "        pname = 'transformer.final_layernorm.bias'\n",
    "        w0, ws0 = find_weight(m0, pname)\n",
    "        w = w0.numpy()\n",
    "        assert ws0 == xs\n",
    "        new_weights.append((x.name, xs, pname, w))\n",
    "    else:\n",
    "        print('BAD', x.name, xs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(new_weights) == len(gpt.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in new_weights:\n",
    "    assert tuple(x[1]) == x[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.set_weights([x[-1] for x in new_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jovyan/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/jovyan/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./cpm-logits-model/assets\n"
     ]
    }
   ],
   "source": [
    "gpt.save('./cpm-logits-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
